{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11005731,"sourceType":"datasetVersion","datasetId":3816617}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport kagglehub\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:42:19.064997Z","iopub.execute_input":"2025-03-12T19:42:19.065319Z","iopub.status.idle":"2025-03-12T19:42:29.463647Z","shell.execute_reply.started":"2025-03-12T19:42:19.065290Z","shell.execute_reply":"2025-03-12T19:42:29.462627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tmdb-movies-dataset-2023-930k-movies/TMDB_movie_dataset_v11.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:44:14.650839Z","iopub.execute_input":"2025-03-12T19:44:14.651293Z","iopub.status.idle":"2025-03-12T19:44:27.662063Z","shell.execute_reply.started":"2025-03-12T19:44:14.651250Z","shell.execute_reply":"2025-03-12T19:44:27.660682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for null values in each column\nnull_counts = df.isnull().sum()\nprint(null_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:44:27.664094Z","iopub.execute_input":"2025-03-12T19:44:27.664447Z","iopub.status.idle":"2025-03-12T19:44:28.568501Z","shell.execute_reply.started":"2025-03-12T19:44:27.664419Z","shell.execute_reply":"2025-03-12T19:44:28.567575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf = df.dropna(subset=['title','overview'])\n# Filter rows where the 'language' column is 'en'\ndf_clean = df[df['original_language'] == 'en']\n\n#check \nen = df_clean['original_language'] == 'en'\nother = df_clean['original_language'] != 'en'\n\nprint(\"english lyrics:\", en.sum())\nprint(\"other language lyrics:\", other.sum())\nnull_counts_lang = df_clean['original_language'].isnull().sum()\nprint(\"null language values:\",null_counts_lang)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:44:28.570280Z","iopub.execute_input":"2025-03-12T19:44:28.570673Z","iopub.status.idle":"2025-03-12T19:44:29.307588Z","shell.execute_reply.started":"2025-03-12T19:44:28.570643Z","shell.execute_reply":"2025-03-12T19:44:29.306630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:44:38.801497Z","iopub.execute_input":"2025-03-12T19:44:38.801894Z","iopub.status.idle":"2025-03-12T19:44:38.808001Z","shell.execute_reply.started":"2025-03-12T19:44:38.801864Z","shell.execute_reply":"2025-03-12T19:44:38.807039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#drop irrelevant columns \ncolumns_to_drop = ['vote_average', 'vote_count', 'status', 'release_date',\n       'revenue', 'runtime', 'adult', 'backdrop_path', 'budget', 'homepage',\n       'imdb_id', 'original_language', 'original_title', \n       'popularity', 'poster_path', 'tagline',\n       'production_companies', 'production_countries', 'spoken_languages',\n       'keywords']\ndf_clean = df_clean.drop(columns=columns_to_drop)\ndf_clean.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:45:14.478812Z","iopub.execute_input":"2025-03-12T19:45:14.479149Z","iopub.status.idle":"2025-03-12T19:45:14.574997Z","shell.execute_reply.started":"2025-03-12T19:45:14.479122Z","shell.execute_reply":"2025-03-12T19:45:14.573991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for null values in each column\nnull_counts = df_clean.isnull().sum()\nprint(null_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:45:29.119798Z","iopub.execute_input":"2025-03-12T19:45:29.120113Z","iopub.status.idle":"2025-03-12T19:45:29.266501Z","shell.execute_reply.started":"2025-03-12T19:45:29.120088Z","shell.execute_reply":"2025-03-12T19:45:29.265455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean.head()\nnum_rows_before = df.shape[0]\nnum_rows = df_clean.shape[0]\nprint(f\"Number of rows in the dataset before: {num_rows_before}\")\nprint(f\"Number of rows in the dataset: {num_rows}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:45:35.731417Z","iopub.execute_input":"2025-03-12T19:45:35.731926Z","iopub.status.idle":"2025-03-12T19:45:35.738832Z","shell.execute_reply.started":"2025-03-12T19:45:35.731887Z","shell.execute_reply":"2025-03-12T19:45:35.736898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n\n# Get a random sample of lyrics\nprint(df_clean['overview'].sample(1).iloc[0])\n\n# Optionally reset column width back to default (optional)\npd.reset_option('display.max_colwidth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:46:02.175013Z","iopub.execute_input":"2025-03-12T19:46:02.175340Z","iopub.status.idle":"2025-03-12T19:46:02.203216Z","shell.execute_reply.started":"2025-03-12T19:46:02.175316Z","shell.execute_reply":"2025-03-12T19:46:02.202129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"import re\n# to reduce vocab size let s try to remove more unecessary symbols and parts \n# Define a cleaning function\ndef clean_overview(overview):\n    # Remove section tags like [Intro], [Verse 1], etc.\n    cleaned = re.sub(r'\\[.*?\\]', '', overview)\n    \n    # Remove credits or text after \"---\"\n    cleaned = re.split(r'---', cleaned)[0]\n    \n    # Remove symbols except for line breaks (\\n) and alphanumeric characters\n    cleaned = re.sub(r'[^\\w\\s\\n]', '', cleaned)\n    \n    # Remove extra whitespace and blank lines\n    cleaned = re.sub(r'\\n\\s*\\n', '\\n', cleaned).strip()\n    \n    return cleaned\n\n# Apply the cleaning function\ndf_clean['overview'] = df_clean['overview'].apply(clean_overview)\nprint(df_clean['overview'].head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:47:06.218300Z","iopub.execute_input":"2025-03-12T19:47:06.218714Z","iopub.status.idle":"2025-03-12T19:47:12.386533Z","shell.execute_reply.started":"2025-03-12T19:47:06.218682Z","shell.execute_reply":"2025-03-12T19:47:12.385638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n# Get a random sample of lyrics\nprint(df_clean['overview'].sample(1).iloc[0])\n# Optionally reset column width back to default (optional)\npd.reset_option('display.max_colwidth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:47:19.487752Z","iopub.execute_input":"2025-03-12T19:47:19.488089Z","iopub.status.idle":"2025-03-12T19:47:19.516223Z","shell.execute_reply.started":"2025-03-12T19:47:19.488064Z","shell.execute_reply":"2025-03-12T19:47:19.515056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization & Embedding\nsentence-transformers/all-mpnet-base-v2 : This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.","metadata":{}},{"cell_type":"code","source":"# Set up device for GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:47:32.441159Z","iopub.execute_input":"2025-03-12T19:47:32.441549Z","iopub.status.idle":"2025-03-12T19:47:32.452373Z","shell.execute_reply.started":"2025-03-12T19:47:32.441494Z","shell.execute_reply":"2025-03-12T19:47:32.451230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:47:40.496646Z","iopub.execute_input":"2025-03-12T19:47:40.496976Z","iopub.status.idle":"2025-03-12T19:47:40.501888Z","shell.execute_reply.started":"2025-03-12T19:47:40.496951Z","shell.execute_reply":"2025-03-12T19:47:40.500790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model from HuggingFace Hub and move to gpu\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = torch.nn.DataParallel(model)\nmodel.to(device)\nmodel.eval()  # Set model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:47:46.399032Z","iopub.execute_input":"2025-03-12T19:47:46.399381Z","iopub.status.idle":"2025-03-12T19:48:06.329893Z","shell.execute_reply.started":"2025-03-12T19:47:46.399354Z","shell.execute_reply":"2025-03-12T19:48:06.328569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to compute embedding\ndef compute_embedding(text):\n    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n    sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n    # Convert to list for Parquet compatibility\n    return sentence_embedding.squeeze().numpy().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:48:06.331207Z","iopub.execute_input":"2025-03-12T19:48:06.331825Z","iopub.status.idle":"2025-03-12T19:48:06.337505Z","shell.execute_reply.started":"2025-03-12T19:48:06.331796Z","shell.execute_reply":"2025-03-12T19:48:06.336281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to compute embeddings in batches\ndef compute_embeddings_batch(texts, batch_size=128):\n    embeddings_list = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        # Tokenize batch of texts\n        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n        # Move tensors to GPU\n        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n        # Compute embeddings without gradient tracking\n        with torch.no_grad():\n            model_output = model(**encoded_input)\n        batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n        batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n        # Move embeddings back to CPU and convert to list\n        embeddings_list.extend(batch_embeddings.cpu().numpy().tolist())\n        if (i // batch_size) % 100 == 0:\n            print(f\"Processed {i + len(batch_texts)} / {len(texts)} lyrics\")\n    return embeddings_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:48:06.339305Z","iopub.execute_input":"2025-03-12T19:48:06.339691Z","iopub.status.idle":"2025-03-12T19:48:06.379913Z","shell.execute_reply.started":"2025-03-12T19:48:06.339664Z","shell.execute_reply":"2025-03-12T19:48:06.378645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To handle 3 million rows, using process in chunks.\n# Process 100,000 rows at a time and append the results to a list.\nchunk_size = 100_000  # adjust based on your available memory and GPU\nembedding_results = []\nchunks = []\nfor start in range(0, len(df_clean), chunk_size):\n    end = min(start + chunk_size, len(df_clean))\n    df_chunk = df_clean.iloc[start:end].copy()\n    print(f\"Processing rows {start} to {end}\")\n    movies_list = df_chunk['overview'].tolist()\n    df_chunk['embedding'] = compute_embeddings_batch(movies_list, batch_size=128)\n    # Append chunk result (consider saving each chunk separately if memory is tight)\n    chunks.append(df_chunk)\n\n# Concatenate all chunks into a final DataFrame\ndf_embeddings = pd.concat(chunks, ignore_index=True)\n\n# Save the DataFrame as a Parquet file.\n# Ensure embeddings are stored as lists of floats.\ndf_embeddings.to_parquet('movies_embeddings.parquet', engine='pyarrow')\nprint(\"Embeddings and metadata saved as a Parquet file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T19:48:34.854786Z","iopub.execute_input":"2025-03-12T19:48:34.855127Z","iopub.status.idle":"2025-03-12T19:49:06.317101Z","shell.execute_reply.started":"2025-03-12T19:48:34.855101Z","shell.execute_reply":"2025-03-12T19:49:06.314655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Stat:\n\nTotal Time 1h 6m 47s Â· GPU T4 x2\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}